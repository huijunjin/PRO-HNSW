{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fdca30-1391-4991-a371-7478540bd8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# PRO-HNSW: Main Performance Evaluation\n",
    "\n",
    "This notebook runs the core experiments to evaluate the performance of PRO-HNSW against the standard HNSW under various dynamic update scenarios.\n",
    "\n",
    "The workflow is as follows:\n",
    "1.  **Configuration**: Set up all parameters for the experiment.\n",
    "2.  **Core Functions**: Define the necessary functions for building, updating, and evaluating the index.\n",
    "3.  **Main Loop**: Iterate through datasets and deletion ratios to run experiments and save results to a CSV file.\n",
    "\"\"\"\n",
    "\n",
    "# ===================================================================\n",
    "# ðŸ“ 1. CONFIGURATION SECTION\n",
    "# All user-configurable parameters are defined here.\n",
    "# ===================================================================\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Global Experiment Settings â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# NOTE: For a quick test run, set BASE_SIZE_LIMIT and QUERY_SIZE_LIMIT to a small number (e.g., 10000).\n",
    "# For the full experiment, set them to None.\n",
    "BASE_SIZE_LIMIT = None\n",
    "QUERY_SIZE_LIMIT = None\n",
    "RECALCULATE_GT = False      # Set to True to re-calculate ground truth (automatically enabled if BASE_SIZE_LIMIT is used)\n",
    "TRIALS_LATENCY = 3          # Number of trials for QPS measurement\n",
    "DELETION_RATIOS = [0.2, 0.8] # List of deletion ratios to test\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ HNSW Parameter Tiers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Defines different sets of HNSW parameters for different dataset sizes.\n",
    "HNSW_PARAMS_TIERS = {\n",
    "    \"small\":  {\"M\": 8,  \"Efc\": 50,  \"rein\": 25, \"efs_range\": (25, 76, 1)},\n",
    "    \"medium\": {\"M\": 12, \"Efc\": 75,  \"rein\": 50, \"efs_range\": (50, 101, 1)},\n",
    "    \"large\":  {\"M\": 16, \"Efc\": 100, \"rein\": 75, \"efs_range\": (75, 126, 1)},\n",
    "}\n",
    "DEFAULT_HNSW_TIER = \"medium\"\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Dataset Profiles â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Maps each dataset to an HNSW parameter tier.\n",
    "DATASET_HNSW_TIER_PROFILE = {\n",
    "    # Large Tier\n",
    "    \"deep-image-96-angular\": \"large\",\n",
    "    \"gist-960-euclidean\": \"large\",\n",
    "    # Medium Tier\n",
    "    \"sift-128-euclidean\": \"medium\",\n",
    "    \"nytimes-256-angular\": \"medium\",\n",
    "    # Small Tier\n",
    "    \"fashion-mnist-784-euclidean\": \"small\",\n",
    "    \"mnist-784-euclidean\": \"small\",\n",
    "    \"coco-i2i-512-angular\": \"small\",\n",
    "    \"glove-25-angular\": \"small\",\n",
    "}\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ File Paths and Constants â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# NOTE: Assumes datasets are in a 'data/' subdirectory and results will be saved to a 'results/' subdirectory.\n",
    "DATA_ROOT_HDF5 = \"../data\"\n",
    "RESULTS_DIR = \"../results/bulk\"\n",
    "N_THREADS = 18\n",
    "BF_CHUNK_SIZE = 600\n",
    "RECALL_K = 10\n",
    "\n",
    "# ===================================================================\n",
    "# ðŸ› ï¸ 2. SETUP & LIBRARY IMPORTS\n",
    "# ===================================================================\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "import heapq\n",
    "import numpy as np\n",
    "import hnswlib\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from datetime import datetime\n",
    "# This assumes you have a 'dataset_loader.py' or similar in a 'dataset' folder.\n",
    "# Adjust the import path if necessary.\n",
    "from data.hdf5_dataset_loader import load_dataset\n",
    "\n",
    "# Prepare results directory\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# ===================================================================\n",
    "# âš™ï¸ 3. DYNAMIC CONFIGURATION GENERATION\n",
    "# This block automatically creates the final experiment configurations.\n",
    "# Do not modify this block directly. Change the settings in Section 1 instead.\n",
    "# ===================================================================\n",
    "\n",
    "CFG_BASE_CONFIG = {\n",
    "    \"sift-128-euclidean\": {\"space\": \"l2\"},\n",
    "    \"gist-960-euclidean\": {\"space\": \"l2\"},\n",
    "    \"deep-image-96-angular\": {\"space\": \"cosine\"},\n",
    "    \"glove-25-angular\": {\"space\": \"cosine\"},\n",
    "    \"mnist-784-euclidean\": {\"space\": \"l2\"},\n",
    "    \"fashion-mnist-784-euclidean\": {\"space\": \"l2\"},\n",
    "    \"nytimes-256-angular\": {\"space\": \"cosine\"},\n",
    "    \"coco-i2i-512-angular\": {\"space\": \"cosine\"},\n",
    "}\n",
    "\n",
    "CFG = {}\n",
    "for dname_key, base_settings in CFG_BASE_CONFIG.items():\n",
    "    tier_name = DATASET_HNSW_TIER_PROFILE.get(dname_key, DEFAULT_HNSW_TIER)\n",
    "    tier_params = HNSW_PARAMS_TIERS.get(tier_name, HNSW_PARAMS_TIERS[DEFAULT_HNSW_TIER])\n",
    "\n",
    "    CFG[dname_key] = {\n",
    "        **base_settings,\n",
    "        **tier_params,\n",
    "        \"efs_list\": list(range(*tier_params[\"efs_range\"])),\n",
    "        \"base_size_limit\": BASE_SIZE_LIMIT,\n",
    "        \"query_size_limit\": QUERY_SIZE_LIMIT,\n",
    "    }\n",
    "\n",
    "print(\"âœ… Configuration generated for the following datasets:\")\n",
    "for dname in CFG.keys():\n",
    "    print(f\"- {dname}\")\n",
    "\n",
    "# ===================================================================\n",
    "# ðŸ”¬ 4. CORE FUNCTIONS\n",
    "# Helper functions for normalization, index building, updating, ground truth calculation, and evaluation.\n",
    "# ===================================================================\n",
    "\n",
    "def normalize_vectors(vectors: np.ndarray) -> np.ndarray:\n",
    "    if vectors is None or vectors.shape[0] == 0: return vectors\n",
    "    if vectors.ndim == 1:\n",
    "        norm = np.linalg.norm(vectors)\n",
    "        return vectors / (norm if norm > 1e-9 else 1e-9)\n",
    "    elif vectors.ndim == 2:\n",
    "        norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "        return vectors / np.where(norms < 1e-9, 1e-9, norms)\n",
    "    else:\n",
    "        raise ValueError(\"normalize_vectors only supports 1D or 2D arrays.\")\n",
    "\n",
    "def build_index(base_vectors: np.ndarray, labels_arr: np.ndarray, M_val: int, Efc_val: int, space_val: str):\n",
    "    dim = base_vectors.shape[1]\n",
    "    print(f\"  Building HNSW index with space='{space_val}', M={M_val}, efC={Efc_val}...\")\n",
    "    idx = hnswlib.Index(space_val, dim)\n",
    "    idx.init_index(max_elements=len(base_vectors),\n",
    "                   ef_construction=Efc_val,\n",
    "                   M=M_val, allow_replace_deleted=True)\n",
    "    \n",
    "    # Ensure data is C-contiguous\n",
    "    current_base_vectors = np.ascontiguousarray(base_vectors, dtype=np.float32)\n",
    "    idx.add_items(current_base_vectors, labels_arr)\n",
    "    print(\"  HNSW index build complete.\")\n",
    "    return idx\n",
    "\n",
    "def update_index(idx: hnswlib.Index, base_vectors: np.ndarray, del_ids_arr: np.ndarray, rein_val: int, optimized=False, dname=\"\", delta_val=0.0):\n",
    "    opt_metrics = {\n",
    "        \"time_roe\": 0.0, \"metric_roe\": 0,\n",
    "        \"time_rdn\": 0.0, \"metric_rdn\": 0,\n",
    "        \"time_rea\": 0.0, \"metric_rea\": 0,\n",
    "    }\n",
    "    \n",
    "    if len(del_ids_arr) > 0:\n",
    "        print(f\"  Marking {len(del_ids_arr)} elements for deletion for {dname} (delta={delta_val})...\")\n",
    "        for d_id in del_ids_arr: idx.mark_deleted(d_id)\n",
    "\n",
    "    if optimized:\n",
    "        t_start_roe = time.time()\n",
    "        removed_count = idx.remove_obsolete_edges(del_ids_arr.tolist() if len(del_ids_arr) > 0 else [])\n",
    "        opt_metrics[\"time_roe\"] = time.time() - t_start_roe\n",
    "        opt_metrics[\"metric_roe\"] = int(removed_count)\n",
    "        if removed_count > 0 or opt_metrics[\"time_roe\"] > 0.001:\n",
    "            print(f\"      [PRO-HNSW] remove_obsolete_edges: {removed_count} edges removed, time: {opt_metrics['time_roe']:.4f}s\")\n",
    "\n",
    "    if len(del_ids_arr) > 0:\n",
    "        print(f\"  Reinserting {len(del_ids_arr)} elements with ef_construction_for_reinsert={rein_val}...\")\n",
    "        idx.set_ef(rein_val)\n",
    "        \n",
    "        shuf = np.random.permutation(del_ids_arr)\n",
    "        reinsert_data = np.ascontiguousarray(base_vectors[shuf], dtype=np.float32)\n",
    "        idx.add_items(reinsert_data, shuf.astype(np.int32), replace_deleted=True)\n",
    "        print(\"  Reinsertion complete.\")\n",
    "    else:\n",
    "        print(\"  No elements marked for deletion, skipping reinsertion.\")\n",
    "\n",
    "    if optimized:\n",
    "        t_start_rdn = time.time()\n",
    "        repaired_nodes_count = idx.repair_disconnected_nodes()\n",
    "        opt_metrics[\"time_rdn\"] = time.time() - t_start_rdn\n",
    "        opt_metrics[\"metric_rdn\"] = int(repaired_nodes_count)\n",
    "        if repaired_nodes_count > 0 or opt_metrics[\"time_rdn\"] > 0.001:\n",
    "            print(f\"      [PRO-HNSW] repair_disconnected_nodes: {repaired_nodes_count} nodes processed, time: {opt_metrics['time_rdn']:.4f}s\")\n",
    "        \n",
    "        t_start_rea = time.time()\n",
    "        resolved_edges_count = idx.resolve_edge_asymmetry()\n",
    "        opt_metrics[\"time_rea\"] = time.time() - t_start_rea\n",
    "        opt_metrics[\"metric_rea\"] = int(resolved_edges_count)\n",
    "        if resolved_edges_count > 0 or opt_metrics[\"time_rea\"] > 0.001:\n",
    "            print(f\"      [PRO-HNSW] resolve_edge_asymmetry: {resolved_edges_count} edges fixed, time: {opt_metrics['time_rea']:.4f}s\")\n",
    "    \n",
    "    return idx, opt_metrics\n",
    "\n",
    "# Ground truth and evaluation functions remain complex, keeping them as is for functionality.\n",
    "# (brute_force_gt_mt, _calculate_scores_for_chunk, eval_index functions would be here)\n",
    "\n",
    "def _calculate_scores_for_chunk(query_vectors_slice: np.ndarray, base_vectors_chunk: np.ndarray, space_type: str) -> np.ndarray:\n",
    "    if space_type == 'l2':\n",
    "        diff = query_vectors_slice[:, np.newaxis, :] - base_vectors_chunk[np.newaxis, :, :]\n",
    "        return np.linalg.norm(diff, axis=2)\n",
    "    elif space_type == 'ip':\n",
    "        return -np.dot(query_vectors_slice, base_vectors_chunk.T)\n",
    "    elif space_type == 'cosine':\n",
    "        q_norm = normalize_vectors(query_vectors_slice)\n",
    "        b_norm = normalize_vectors(base_vectors_chunk)\n",
    "        if q_norm is None or b_norm is None or q_norm.shape[0] == 0 or b_norm.shape[0] == 0:\n",
    "            return np.full((query_vectors_slice.shape[0], base_vectors_chunk.shape[0]), np.inf, dtype=np.float32)\n",
    "        similarities = np.dot(q_norm, b_norm.T)\n",
    "        return 1.0 - np.clip(similarities, -1.0, 1.0)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported space_type for GT calculation: '{space_type}'\")\n",
    "\n",
    "def brute_force_gt_mt(base_vectors: np.ndarray, query_vectors: np.ndarray, k_val: int, space_type: str, dname_for_log: str):\n",
    "    print(f\"    Calculating brute-force GT for {dname_for_log} (k={k_val}, space='{space_type}')...\")\n",
    "    num_queries, num_base = query_vectors.shape[0], base_vectors.shape[0]\n",
    "    k_val = min(k_val, num_base)\n",
    "    if k_val == 0: return np.array([], dtype=np.int32).reshape(num_queries, 0)\n",
    "\n",
    "    gt_indices = np.full((num_queries, k_val), -1, dtype=np.int32)\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=N_THREADS) as executor:\n",
    "        def process_query_chunk(start_idx, end_idx):\n",
    "            scores = _calculate_scores_for_chunk(query_vectors[start_idx:end_idx], base_vectors, space_type)\n",
    "            # Use argpartition for efficiency: find k-th smallest element, then sort only those k\n",
    "            partition_indices = np.argpartition(scores, k_val, axis=1)[:, :k_val]\n",
    "            \n",
    "            # Now, sort within the top-k candidates for each query\n",
    "            for i in range(partition_indices.shape[0]):\n",
    "                query_idx = start_idx + i\n",
    "                top_k_indices_for_query = partition_indices[i]\n",
    "                top_k_scores = scores[i, top_k_indices_for_query]\n",
    "                sorted_within_top_k = np.argsort(top_k_scores)\n",
    "                gt_indices[query_idx, :] = top_k_indices_for_query[sorted_within_top_k]\n",
    "\n",
    "        query_chunks = [(i, min(i + BF_CHUNK_SIZE, num_queries)) for i in range(0, num_queries, BF_CHUNK_SIZE)]\n",
    "        executor.map(lambda p: process_query_chunk(*p), query_chunks)\n",
    "\n",
    "    print(f\"    Brute-force GT calculation for {dname_for_log} complete.\")\n",
    "    return gt_indices\n",
    "\n",
    "\n",
    "def eval_index(idx: hnswlib.Index, query_vectors: np.ndarray, gt_indices: np.ndarray, ef_search: int, num_trials: int, recall_at_k: int):\n",
    "    if query_vectors is None or query_vectors.shape[0] == 0: return 0.0, 0.0\n",
    "    if gt_indices is None or gt_indices.shape[0] != query_vectors.shape[0]:\n",
    "        # Cannot calculate recall, just measure QPS\n",
    "        total_time = 0.0\n",
    "        idx.set_ef(ef_search)\n",
    "        for _ in range(num_trials):\n",
    "            start_time = time.time()\n",
    "            idx.knn_query(query_vectors, k=recall_at_k if recall_at_k > 0 else 1)\n",
    "            total_time += time.time() - start_time\n",
    "        qps = (num_trials * len(query_vectors) / total_time) if total_time > 0 else 0.0\n",
    "        return 0.0, qps\n",
    "    \n",
    "    idx.set_ef(ef_search)\n",
    "    total_time, total_correct_hits = 0.0, 0\n",
    "    num_queries = len(query_vectors)\n",
    "    k_to_evaluate = min(recall_at_k, gt_indices.shape[1])\n",
    "\n",
    "    if k_to_evaluate == 0: return 0.0, 0.0\n",
    "\n",
    "    gt_sets_for_eval = [set(row[:k_to_evaluate]) for row in gt_indices]\n",
    "\n",
    "    for _ in range(num_trials):\n",
    "        start_time = time.time()\n",
    "        found_indices_batch, _ = idx.knn_query(query_vectors, k=k_to_evaluate)\n",
    "        total_time += time.time() - start_time\n",
    "        for i in range(num_queries):\n",
    "            common_hits = len(set(found_indices_batch[i]) & gt_sets_for_eval[i])\n",
    "            total_correct_hits += common_hits\n",
    "            \n",
    "    qps = (num_trials * num_queries / total_time) if total_time > 0 else 0.0\n",
    "    total_possible_hits = num_trials * sum(len(s) for s in gt_sets_for_eval)\n",
    "    recall = (total_correct_hits / total_possible_hits) * 100.0 if total_possible_hits > 0 else 0.0\n",
    "    return recall, qps\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# ðŸš€ 5. MAIN EXPERIMENT LOOP\n",
    "# ===================================================================\n",
    "\n",
    "# Prepare CSV file\n",
    "csv_path = f\"{RESULTS_DIR}/experiment_results_{TIMESTAMP}.csv\"\n",
    "csv_header = [\n",
    "    \"timestamp\", \"dataset\", \"delta\", \"variant\", \"ef_search\",\n",
    "    \"M\", \"Efc\", \"rein\", \"space\", \"recall_k\",\n",
    "    \"qps\", \"recall\",\n",
    "    \"time_roe\", \"metric_roe\",\n",
    "    \"time_rdn\", \"metric_rdn\",\n",
    "    \"time_rea\", \"metric_rea\",\n",
    "    \"time_idx_build_total\",\n",
    "    \"time_idx_update_total\",\n",
    "    \"base_size_used\", \"query_size_used\"\n",
    "]\n",
    "with open(csv_path, \"w\", newline=\"\") as fp:\n",
    "    csv.writer(fp).writerow(csv_header)\n",
    "\n",
    "print(f\"\\nðŸš€ Starting experiments. Results will be saved to: {csv_path}\\n\")\n",
    "\n",
    "for dname_key, current_cfg in CFG.items():\n",
    "    effective_recalculate_gt = RECALCULATE_GT or (current_cfg[\"base_size_limit\"] is not None)\n",
    "\n",
    "    print(f\"\\n{'='*20} Processing Dataset: {dname_key.upper()} ({current_cfg['space']}) {'='*20}\")\n",
    "    print(f\"  Config: M={current_cfg['M']}, Efc={current_cfg['Efc']}, Rein={current_cfg['rein']}\")\n",
    "    print(f\"  Size Limits: Base={current_cfg['base_size_limit']}, Query={current_cfg['query_size_limit']}\")\n",
    "    print(f\"  Settings: Recalculate_GT={effective_recalculate_gt}, Latency_Trials={TRIALS_LATENCY}\")\n",
    "\n",
    "    try:\n",
    "        base_data_full, Q_data_full, gt_provided_hdf5 = load_dataset(dname_key, DATA_ROOT_HDF5)\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR: Could not load dataset '{dname_key}': {e}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    if base_data_full is None or base_data_full.shape[0] == 0: continue\n",
    "    \n",
    "    # Slice datasets if limits are set\n",
    "    base_data_sliced = base_data_full[:current_cfg[\"base_size_limit\"]] if current_cfg[\"base_size_limit\"] is not None else base_data_full\n",
    "    actual_base_size = base_data_sliced.shape[0]\n",
    "    if actual_base_size == 0: continue\n",
    "    labels_for_index = np.arange(actual_base_size, dtype=np.int32)\n",
    "    \n",
    "    Q_data_sliced = Q_data_full[:current_cfg[\"query_size_limit\"]] if current_cfg[\"query_size_limit\"] is not None and Q_data_full is not None else Q_data_full\n",
    "    actual_query_size = Q_data_sliced.shape[0] if Q_data_sliced is not None else 0\n",
    "\n",
    "    if actual_query_size == 0:\n",
    "        print(f\"  WARNING: No query data available for '{dname_key}'. Skipping evaluation part.\")\n",
    "        continue\n",
    "\n",
    "    # Prepare Ground Truth\n",
    "    gt_to_use = None\n",
    "    if effective_recalculate_gt:\n",
    "        print(f\"  Recalculating Ground Truth for '{dname_key}'...\")\n",
    "        gt_to_use = brute_force_gt_mt(base_data_sliced, Q_data_sliced, RECALL_K, current_cfg[\"space\"], dname_key)\n",
    "    elif gt_provided_hdf5 is not None:\n",
    "        print(f\"  Using pre-computed Ground Truth from HDF5 file for '{dname_key}'.\")\n",
    "        gt_to_use = gt_provided_hdf5[:actual_query_size]\n",
    "    else:\n",
    "        print(f\"  WARNING: No Ground Truth found and recalculation is off. Recalculating anyway for '{dname_key}'...\")\n",
    "        gt_to_use = brute_force_gt_mt(base_data_sliced, Q_data_sliced, RECALL_K, current_cfg[\"space\"], dname_key)\n",
    "\n",
    "    if gt_to_use is None or gt_to_use.shape[0] != actual_query_size:\n",
    "        print(f\"  FATAL ERROR: Could not prepare Ground Truth for '{dname_key}'. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"  Final sizes: Base={actual_base_size}, Query={actual_query_size}, GT_K={gt_to_use.shape[1] if gt_to_use.ndim == 2 else 'N/A'}\")\n",
    "\n",
    "    for delta_val in DELETION_RATIOS:\n",
    "        print(f\"\\n-- Running for Dataset: {dname_key.upper()}, Deletion Ratio (Î´) = {delta_val} --\")\n",
    "        num_to_delete = int(actual_base_size * delta_val)\n",
    "        if num_to_delete > actual_base_size:\n",
    "            print(f\"  Skipping delta={delta_val} as base size {actual_base_size} is too small to delete {num_to_delete} items.\")\n",
    "            continue\n",
    "        del_ids_arr = np.random.choice(labels_for_index, num_to_delete, replace=False) if num_to_delete > 0 else np.array([], dtype=np.int32)\n",
    "\n",
    "        # --- Run for Original HNSW ---\n",
    "        print(f\"\\n  Processing Original HNSW...\")\n",
    "        t_build_start = time.time()\n",
    "        idx_orig_build = build_index(base_data_sliced, labels_for_index, current_cfg['M'], current_cfg['Efc'], current_cfg['space'])\n",
    "        time_idx_build_original = time.time() - t_build_start\n",
    "        print(f\"    Original HNSW index built in {time_idx_build_original:.4f}s\")\n",
    "\n",
    "        t_update_start = time.time()\n",
    "        idx_orig, _ = update_index(idx_orig_build, base_data_sliced, del_ids_arr, current_cfg['rein'], optimized=False, dname=dname_key, delta_val=delta_val)\n",
    "        time_idx_update_original = time.time() - t_update_start\n",
    "        print(f\"    Original HNSW index updated in {time_idx_update_original:.4f}s\")\n",
    "        \n",
    "        print(f\"  Evaluating Original HNSW (Î´={delta_val})...\")\n",
    "        for ef_search_val in current_cfg[\"efs_list\"]:\n",
    "            rec, qps = eval_index(idx_orig, Q_data_sliced, gt_to_use, ef_search_val, TRIALS_LATENCY, RECALL_K)\n",
    "            print(f\"    original ef={ef_search_val:3d}  R={rec:5.2f}%  QPS={qps:9.1f}\")\n",
    "            row_data = [TIMESTAMP, dname_key, delta_val, \"original\", ef_search_val, *current_cfg.values(), f\"{qps:.3f}\", f\"{rec:.2f}\", 0,0,0,0,0,0, f\"{time_idx_build_original:.4f}\", f\"{time_idx_update_original:.4f}\", actual_base_size, actual_query_size]\n",
    "            with open(csv_path, \"a\", newline=\"\") as fp: csv.writer(fp).writerow(row_data)\n",
    "        del idx_orig_build, idx_orig\n",
    "        \n",
    "        # --- Run for Optimized (PRO-HNSW) ---\n",
    "        print(f\"\\n  Processing PRO-HNSW...\")\n",
    "        t_build_start = time.time()\n",
    "        idx_opt_build = build_index(base_data_sliced, labels_for_index, current_cfg['M'], current_cfg['Efc'], current_cfg['space'])\n",
    "        time_idx_build_optimized = time.time() - t_build_start\n",
    "        print(f\"    PRO-HNSW index built in {time_idx_build_optimized:.4f}s\")\n",
    "\n",
    "        t_update_start = time.time()\n",
    "        idx_opt, opt_metrics = update_index(idx_opt_build, base_data_sliced, del_ids_arr, current_cfg['rein'], optimized=True, dname=dname_key, delta_val=delta_val)\n",
    "        time_idx_update_optimized = time.time() - t_update_start\n",
    "        print(f\"    PRO-HNSW index updated in {time_idx_update_optimized:.4f}s\")\n",
    "        \n",
    "        print(f\"  Evaluating PRO-HNSW (Î´={delta_val})...\")\n",
    "        for ef_search_val in current_cfg[\"efs_list\"]:\n",
    "            rec, qps = eval_index(idx_opt, Q_data_sliced, gt_to_use, ef_search_val, TRIALS_LATENCY, RECALL_K)\n",
    "            print(f\"    pro-hnsw ef={ef_search_val:3d}  R={rec:5.2f}%  QPS={qps:9.1f}\")\n",
    "            row_data = [TIMESTAMP, dname_key, delta_val, \"pro-hnsw\", ef_search_val, *current_cfg.values(), f\"{qps:.3f}\", f\"{rec:.2f}\",\n",
    "                        f\"{opt_metrics.get('time_roe', 0.0):.4f}\", opt_metrics.get('metric_roe', 0),\n",
    "                        f\"{opt_metrics.get('time_rdn', 0.0):.4f}\", opt_metrics.get('metric_rdn', 0),\n",
    "                        f\"{opt_metrics.get('time_rea', 0.0):.4f}\", opt_metrics.get('metric_rea', 0),\n",
    "                        f\"{time_idx_build_optimized:.4f}\", f\"{time_idx_update_optimized:.4f}\",\n",
    "                        actual_base_size, actual_query_size]\n",
    "            with open(csv_path, \"a\", newline=\"\") as fp: csv.writer(fp).writerow(row_data)\n",
    "        del idx_opt_build, idx_opt\n",
    "\n",
    "        import gc\n",
    "        gc.collect()\n",
    "        time.sleep(1) # Give a moment for memory to be released\n",
    "\n",
    "print(f\"\\nâœ… All experiments complete. Final results have been saved to: {csv_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
