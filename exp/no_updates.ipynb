{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9790944d-d246-4107-be78-57813c4de0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# PRO-HNSW: Static Graph Optimization Experiment (No Updates)\n",
    "\n",
    "This notebook evaluates the effectiveness of the PRO-HNSW repair modules\n",
    "(repair_disconnected_nodes and resolve_edge_asymmetry) on a freshly built,\n",
    "static HNSW index without any delete or update operations.\n",
    "\n",
    "The workflow is as follows:\n",
    "1.  Build a standard HNSW index.\n",
    "2.  Evaluate its performance (\"baseline_initial\").\n",
    "3.  Apply the PRO-HNSW repair functions to the *same* index.\n",
    "4.  Evaluate its performance again (\"optimized_static\").\n",
    "5.  Save results to a CSV file for comparison.\n",
    "\"\"\"\n",
    "\n",
    "# ===================================================================\n",
    "# 📝 1. CONFIGURATION SECTION\n",
    "# All user-configurable parameters are defined here.\n",
    "# ===================================================================\n",
    "\n",
    "# ────────── Global Experiment Settings ──────────\n",
    "BASE_SIZE_LIMIT = None  # Set to a number (e.g., 10000) for a quick test, or None for full dataset.\n",
    "QUERY_SIZE_LIMIT = None\n",
    "RECALCULATE_GT = False # Not typically needed if HDF5 provides GT.\n",
    "TRIALS_LATENCY = 3\n",
    "\n",
    "# ────────── HNSW Parameter Tiers ──────────\n",
    "HNSW_PARAMS_TIERS = {\n",
    "    \"small\":  {\"M\": 8,  \"Efc\": 50,  \"efs_range\": (25, 76, 10)},\n",
    "    \"medium\": {\"M\": 12, \"Efc\": 75,  \"efs_range\": (50, 101, 10)},\n",
    "    \"large\":  {\"M\": 16, \"Efc\": 100, \"efs_range\": (75, 126, 10)},\n",
    "}\n",
    "DEFAULT_HNSW_TIER = \"medium\"\n",
    "\n",
    "# ────────── Dataset Profiles ──────────\n",
    "DATASET_HNSW_TIER_PROFILE = {\n",
    "    \"deep-image-96-angular\": \"large\",\n",
    "    \"gist-960-euclidean\": \"large\",\n",
    "    \"sift-128-euclidean\": \"medium\",\n",
    "    \"nytimes-256-angular\": \"medium\",\n",
    "    \"fashion-mnist-784-euclidean\": \"small\",\n",
    "    \"mnist-784-euclidean\": \"small\",\n",
    "    \"coco-i2i-512-angular\": \"small\",\n",
    "    \"glove-25-angular\": \"small\",\n",
    "}\n",
    "\n",
    "# ────────── File Paths and Constants ──────────\n",
    "DATA_ROOT_HDF5 = \"../data\"\n",
    "RESULTS_DIR = \"../results/no_updates\"\n",
    "N_THREADS = 18\n",
    "RECALL_K = 10\n",
    "\n",
    "# ===================================================================\n",
    "# 🛠️ 2. SETUP & LIBRARY IMPORTS\n",
    "# ===================================================================\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "import numpy as np\n",
    "import hnswlib\n",
    "import h5py\n",
    "import gc\n",
    "from datetime import datetime\n",
    "# This assumes you have a 'dataset_loader.py' or similar.\n",
    "# For simplicity, a local loader is defined in the functions below.\n",
    "\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# ===================================================================\n",
    "# ⚙️ 3. DYNAMIC CONFIGURATION GENERATION\n",
    "# ===================================================================\n",
    "\n",
    "CFG_BASE_CONFIG = {\n",
    "    \"sift-128-euclidean\": {\"space\": \"l2\"},\n",
    "    \"gist-960-euclidean\": {\"space\": \"l2\"},\n",
    "    \"deep-image-96-angular\": {\"space\": \"cosine\"},\n",
    "    \"glove-25-angular\": {\"space\": \"cosine\"},\n",
    "    \"mnist-784-euclidean\": {\"space\": \"l2\"},\n",
    "    \"fashion-mnist-784-euclidean\": {\"space\": \"l2\"},\n",
    "    \"nytimes-256-angular\": {\"space\": \"cosine\"},\n",
    "    \"coco-i2i-512-angular\": {\"space\": \"cosine\"},\n",
    "}\n",
    "\n",
    "CFG = {}\n",
    "for dname_key, base_settings in CFG_BASE_CONFIG.items():\n",
    "    tier_name = DATASET_HNSW_TIER_PROFILE.get(dname_key, DEFAULT_HNSW_TIER)\n",
    "    tier_params = HNSW_PARAMS_TIERS.get(tier_name, HNSW_PARAMS_TIERS[DEFAULT_HNSW_TIER])\n",
    "    CFG[dname_key] = {\n",
    "        **base_settings,\n",
    "        \"M\": tier_params[\"M\"],\n",
    "        \"Efc\": tier_params[\"Efc\"],\n",
    "        \"efs_list\": list(range(*tier_params[\"efs_range\"])),\n",
    "        \"base_size_limit\": BASE_SIZE_LIMIT,\n",
    "        \"query_size_limit\": QUERY_SIZE_LIMIT,\n",
    "    }\n",
    "\n",
    "print(\"✅ Configuration generated for the following datasets:\")\n",
    "for dname in CFG.keys():\n",
    "    print(f\"- {dname}\")\n",
    "\n",
    "# ===================================================================\n",
    "# 🔬 4. CORE HELPER FUNCTIONS\n",
    "# ===================================================================\n",
    "\n",
    "def normalize_vectors(vectors: np.ndarray, space_type: str) -> np.ndarray:\n",
    "    if space_type != 'cosine' or vectors is None or vectors.size == 0:\n",
    "        return vectors.astype(np.float32) if vectors is not None else None\n",
    "    vectors_float32 = vectors.astype(np.float32)\n",
    "    norms = np.linalg.norm(vectors_float32, axis=1, keepdims=True)\n",
    "    return vectors_float32 / np.where(norms == 0, 1e-9, norms)\n",
    "\n",
    "def load_hdf5_data(data_root, dataset_name, recall_k):\n",
    "    file_path = os.path.join(data_root, f\"{dataset_name}.hdf5\")\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"ERROR: Dataset file not found at {file_path}\")\n",
    "        return None, None, None, None\n",
    "    try:\n",
    "        with h5py.File(file_path, 'r') as f:\n",
    "            base = np.array(f['train'], dtype=np.float32)\n",
    "            query = np.array(f['test'], dtype=np.float32)\n",
    "            gt = np.array(f['neighbors'])\n",
    "            space = 'l2'\n",
    "            if 'angular' in dataset_name.lower() or 'cosine' in dataset_name.lower():\n",
    "                space = 'cosine'\n",
    "\n",
    "            if gt.ndim == 2 and gt.shape[1] > recall_k:\n",
    "                gt = gt[:, :recall_k]\n",
    "            \n",
    "            print(f\"Dataset '{dataset_name}' loaded: Base {base.shape}, Query {query.shape}, GT {gt.shape}, Space: {space}\")\n",
    "            return base, query, gt, space\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading HDF5 file {file_path}: {e}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "def build_index(base_vectors: np.ndarray, labels_arr: np.ndarray, M, Efc, space, threads):\n",
    "    dim = base_vectors.shape[1]\n",
    "    print(f\"  Building HNSW index (Space: {space}, M: {M}, efC: {Efc})...\")\n",
    "    idx = hnswlib.Index(space=space, dim=dim)\n",
    "    idx.init_index(max_elements=len(base_vectors), ef_construction=Efc, M=M)\n",
    "    \n",
    "    if hasattr(idx, 'set_num_threads'):\n",
    "        idx.set_num_threads(threads)\n",
    "    \n",
    "    idx.add_items(np.ascontiguousarray(base_vectors), labels_arr)\n",
    "    print(f\"  HNSW index build complete.\")\n",
    "    return idx\n",
    "\n",
    "def evaluate_performance(idx: hnswlib.Index, query_vectors: np.ndarray, gt_indices: np.ndarray, ef_search, num_trials, recall_k):\n",
    "    if query_vectors is None or gt_indices is None or query_vectors.shape[0] != gt_indices.shape[0]:\n",
    "        return 0.0, 0.0\n",
    "    \n",
    "    idx.set_ef(ef_search)\n",
    "    total_time, total_correct_hits = 0.0, 0\n",
    "    num_queries = query_vectors.shape[0]\n",
    "    k_eval = min(recall_k, gt_indices.shape[1])\n",
    "    gt_sets = [set(row[:k_eval]) for row in gt_indices]\n",
    "\n",
    "    for _ in range(num_trials):\n",
    "        start_time = time.time()\n",
    "        labels_batch, _ = idx.knn_query(query_vectors, k=k_eval)\n",
    "        total_time += time.time() - start_time\n",
    "        if labels_batch.shape[0] == num_queries:\n",
    "            for i in range(num_queries):\n",
    "                total_correct_hits += len(set(labels_batch[i]) & gt_sets[i])\n",
    "    \n",
    "    qps = (num_trials * num_queries / total_time) if total_time > 0 else 0.0\n",
    "    total_gt_count = sum(len(s) for s in gt_sets) * num_trials\n",
    "    recall = (total_correct_hits / total_gt_count) * 100.0 if total_gt_count > 0 else 0.0\n",
    "    return recall, qps\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# 🚀 5. MAIN EXPERIMENT LOOP\n",
    "# ===================================================================\n",
    "csv_path = f\"{RESULTS_DIR}/experiment_results_static_{TIMESTAMP}.csv\"\n",
    "csv_header = [\n",
    "    \"timestamp\", \"dataset\", \"variant\", \"ef_search\",\n",
    "    \"M\", \"Efc\", \"space\", \"recall_k\",\n",
    "    \"qps\", \"recall\",\n",
    "    \"time_idx_build\", \"time_apply_rdn\", \"metric_rdn\", \"time_apply_rea\", \"metric_rea\"\n",
    "]\n",
    "with open(csv_path, \"w\", newline=\"\") as fp:\n",
    "    csv.writer(fp).writerow(csv_header)\n",
    "\n",
    "print(f\"\\n🚀 Starting static optimization experiments. Results will be saved to: {csv_path}\\n\")\n",
    "\n",
    "for dname_key, current_cfg in CFG.items():\n",
    "    print(f\"\\n{'='*20} Processing Dataset: {dname_key.upper()} ({current_cfg['space']}) {'='*20}\")\n",
    "    \n",
    "    # --- Data Preparation ---\n",
    "    base_data, query_data, gt_data, space = load_hdf5_data(DATA_ROOT_HDF5, dname_key, RECALL_K)\n",
    "    if base_data is None: continue\n",
    "\n",
    "    base_data = base_data[:current_cfg[\"base_size_limit\"]] if current_cfg[\"base_size_limit\"] else base_data\n",
    "    query_data = query_data[:current_cfg[\"query_size_limit\"]] if current_cfg[\"query_size_limit\"] else query_data\n",
    "    gt_data = gt_data[:query_data.shape[0]]\n",
    "    \n",
    "    if base_data.size == 0 or query_data.size == 0 or gt_data.size == 0:\n",
    "        print(f\"  Data is empty for {dname_key} after slicing. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    base_data_processed = normalize_vectors(base_data, space)\n",
    "    query_data_processed = normalize_vectors(query_data, space)\n",
    "    labels_for_index = np.arange(base_data_processed.shape[0], dtype=np.int32)\n",
    "    \n",
    "    print(f\"  Data Ready: Base={base_data_processed.shape[0]}, Query={query_data_processed.shape[0]}\")\n",
    "\n",
    "    # --- Build a single, shared index ---\n",
    "    print(f\"  Building a single HNSW index for both variants...\")\n",
    "    t_build_start = time.time()\n",
    "    idx_shared = build_index(base_data_processed, labels_for_index, current_cfg['M'], current_cfg['Efc'], space, N_THREADS)\n",
    "    time_build_common = time.time() - t_build_start\n",
    "    print(f\"  Index built in {time_build_common:.4f}s\")\n",
    "    \n",
    "    # --- 1. Evaluate Baseline (Initial Build) ---\n",
    "    print(\"\\n  Evaluating: Baseline HNSW (as-built)...\")\n",
    "    for ef_search in current_cfg[\"efs_list\"]:\n",
    "        rec, qps = evaluate_performance(idx_shared, query_data_processed, gt_data, ef_search, TRIALS_LATENCY, RECALL_K)\n",
    "        print(f\"    baseline ef={ef_search:3d}  R={rec:5.2f}%  QPS={qps:9.1f}\")\n",
    "        row = [TIMESTAMP, dname_key, \"baseline_initial\", ef_search, current_cfg[\"M\"], current_cfg[\"Efc\"], space, RECALL_K,\n",
    "               f\"{qps:.3f}\", f\"{rec:.4f}\", f\"{time_build_common:.4f}\", 0, 0, 0, 0]\n",
    "        with open(csv_path, \"a\", newline=\"\") as fp: csv.writer(fp).writerow(row)\n",
    "        \n",
    "    # --- 2. Apply Optimizations and Evaluate Again ---\n",
    "    print(\"\\n  Applying PRO-HNSW static optimizations...\")\n",
    "    \n",
    "    time_rdn, metric_rdn, time_rea, metric_rea = 0.0, 0, 0.0, 0\n",
    "    \n",
    "    if hasattr(idx_shared, 'repair_disconnected_nodes'):\n",
    "        t_rdn_start = time.time()\n",
    "        metric_rdn = idx_shared.repair_disconnected_nodes()\n",
    "        time_rdn = time.time() - t_rdn_start\n",
    "        print(f\"    repair_disconnected_nodes completed in {time_rdn:.4f}s, processed {metric_rdn} nodes.\")\n",
    "\n",
    "    if hasattr(idx_shared, 'resolve_edge_asymmetry'):\n",
    "        t_rea_start = time.time()\n",
    "        metric_rea = idx_shared.resolve_edge_asymmetry()\n",
    "        time_rea = time.time() - t_rea_start\n",
    "        print(f\"    resolve_edge_asymmetry completed in {time_rea:.4f}s, fixed {metric_rea} edges.\")\n",
    "    \n",
    "    print(\"\\n  Evaluating: PRO-HNSW (statically optimized)...\")\n",
    "    for ef_search in current_cfg[\"efs_list\"]:\n",
    "        rec, qps = evaluate_performance(idx_shared, query_data_processed, gt_data, ef_search, TRIALS_LATENCY, RECALL_K)\n",
    "        print(f\"    optimized ef={ef_search:3d}  R={rec:5.2f}%  QPS={qps:9.1f}\")\n",
    "        row = [TIMESTAMP, dname_key, \"optimized_static\", ef_search, current_cfg[\"M\"], current_cfg[\"Efc\"], space, RECALL_K,\n",
    "               f\"{qps:.3f}\", f\"{rec:.4f}\", f\"{time_build_common:.4f}\", \n",
    "               f\"{time_rdn:.4f}\", metric_rdn, f\"{time_rea:.4f}\", metric_rea]\n",
    "        with open(csv_path, \"a\", newline=\"\") as fp: csv.writer(fp).writerow(row)\n",
    "\n",
    "    del idx_shared\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"\\n✅ All 'No Update' experiments complete. Results saved to: {csv_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
