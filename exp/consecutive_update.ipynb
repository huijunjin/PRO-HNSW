{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f17b67-0196-4f92-8c5f-fe3f3904e57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# PRO-HNSW: Consecutive Updates Performance Evaluation\n",
    "\n",
    "This notebook runs the experiments for the consecutive update scenario.\n",
    "It simulates a continuous stream of small-batch delete-and-insert operations\n",
    "and evaluates the performance of PRO-HNSW against the standard HNSW.\n",
    "\n",
    "The workflow is as follows:\n",
    "1.  **Configuration**: Set up all parameters for the experiment.\n",
    "2.  **Core Functions**: Define helper functions for the experiment.\n",
    "3.  **Main Loop**: Iterate through datasets, deletion ratios, and variants to run the\n",
    "    consecutive update simulation and save results to a CSV file.\n",
    "\"\"\"\n",
    "\n",
    "# ===================================================================\n",
    "# ðŸ“ 1. CONFIGURATION SECTION\n",
    "# All user-configurable parameters are defined here.\n",
    "# ===================================================================\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Global Experiment Settings â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# NOTE: For a quick test run, set BASE_SIZE_LIMIT and QUERY_SIZE_LIMIT to a small number (e.g., 10000).\n",
    "# For the full experiment, set them to None.\n",
    "BASE_SIZE_LIMIT = None\n",
    "QUERY_SIZE_LIMIT = None\n",
    "RECALCULATE_GT = False\n",
    "TRIALS_LATENCY = 3\n",
    "# Total deletion ratio to reach by the end of the consecutive updates.\n",
    "DELETION_RATIOS = [0.2, 0.8]\n",
    "# Number of small-batch updates to perform to reach the final deletion ratio.\n",
    "N_CONSECUTIVE_UPDATES = 1000\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ HNSW Parameter Tiers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "HNSW_PARAMS_TIERS = {\n",
    "    \"small\":  {\"M\": 8,  \"Efc\": 50,  \"rein\": 25, \"efs_range\": (25, 76, 10)},\n",
    "    \"medium\": {\"M\": 12, \"Efc\": 75,  \"rein\": 50, \"efs_range\": (50, 101, 10)},\n",
    "    \"large\":  {\"M\": 16, \"Efc\": 100, \"rein\": 75, \"efs_range\": (75, 126, 10)},\n",
    "}\n",
    "DEFAULT_HNSW_TIER = \"medium\"\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Dataset Profiles â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "DATASET_HNSW_TIER_PROFILE = {\n",
    "    \"deep-image-96-angular\": \"large\",\n",
    "    \"gist-960-euclidean\": \"large\",\n",
    "    \"sift-128-euclidean\": \"medium\",\n",
    "    \"nytimes-256-angular\": \"medium\",\n",
    "    \"fashion-mnist-784-euclidean\": \"small\",\n",
    "    \"mnist-784-euclidean\": \"small\",\n",
    "    \"coco-i2i-512-angular\": \"small\",\n",
    "    \"glove-25-angular\": \"small\",\n",
    "}\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ File Paths and Constants â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "DATA_ROOT_HDF5 = \"../data\"\n",
    "RESULTS_DIR = \"../results/consecutive\"\n",
    "N_THREADS = 18\n",
    "BF_CHUNK_SIZE = 600\n",
    "RECALL_K = 10\n",
    "\n",
    "# ===================================================================\n",
    "# ðŸ› ï¸ 2. SETUP & LIBRARY IMPORTS\n",
    "# ===================================================================\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "import heapq\n",
    "import numpy as np\n",
    "import hnswlib\n",
    "import gc\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from datetime import datetime\n",
    "# This assumes you have a 'dataset_loader.py' or similar in a 'dataset' folder.\n",
    "from dataset.hdf5_dataset_loader import load_dataset\n",
    "\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# ===================================================================\n",
    "# âš™ï¸ 3. DYNAMIC CONFIGURATION GENERATION\n",
    "# ===================================================================\n",
    "\n",
    "CFG_BASE_CONFIG = {\n",
    "    \"sift-128-euclidean\": {\"space\": \"l2\"},\n",
    "    \"gist-960-euclidean\": {\"space\": \"l2\"},\n",
    "    \"deep-image-96-angular\": {\"space\": \"cosine\"},\n",
    "    \"glove-25-angular\": {\"space\": \"cosine\"},\n",
    "    \"mnist-784-euclidean\": {\"space\": \"l2\"},\n",
    "    \"fashion-mnist-784-euclidean\": {\"space\": \"l2\"},\n",
    "    \"nytimes-256-angular\": {\"space\": \"cosine\"},\n",
    "    \"coco-i2i-512-angular\": {\"space\": \"cosine\"},\n",
    "}\n",
    "\n",
    "CFG = {}\n",
    "for dname_key, base_settings in CFG_BASE_CONFIG.items():\n",
    "    tier_name = DATASET_HNSW_TIER_PROFILE.get(dname_key, DEFAULT_HNSW_TIER)\n",
    "    tier_params = HNSW_PARAMS_TIERS.get(tier_name, HNSW_PARAMS_TIERS[DEFAULT_HNSW_TIER])\n",
    "    CFG[dname_key] = {\n",
    "        **base_settings,\n",
    "        **tier_params,\n",
    "        \"efs_list\": list(range(*tier_params[\"efs_range\"])),\n",
    "        \"base_size_limit\": BASE_SIZE_LIMIT,\n",
    "        \"query_size_limit\": QUERY_SIZE_LIMIT,\n",
    "    }\n",
    "\n",
    "print(\"âœ… Configuration generated for the following datasets:\")\n",
    "for dname in CFG.keys():\n",
    "    print(f\"- {dname}\")\n",
    "\n",
    "# ===================================================================\n",
    "# ðŸ”¬ 4. CORE FUNCTIONS\n",
    "# ===================================================================\n",
    "\n",
    "def normalize_vectors(vectors: np.ndarray) -> np.ndarray:\n",
    "    if vectors is None or vectors.size == 0: return vectors\n",
    "    if vectors.ndim == 1:\n",
    "        norm = np.linalg.norm(vectors)\n",
    "        return vectors / (norm if norm > 1e-9 else 1e-9)\n",
    "    norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    return vectors / np.where(norms < 1e-9, 1e-9, norms)\n",
    "\n",
    "def build_index(base_vectors: np.ndarray, labels_arr: np.ndarray, M_val: int, Efc_val: int, space_val: str, dname_for_log=\"\"):\n",
    "    dim = base_vectors.shape[1]\n",
    "    num_elements = len(base_vectors)\n",
    "    log_prefix = f\"  Build_Index ({dname_for_log}):\" if dname_for_log else \"  Build_Index:\"\n",
    "    print(f\"{log_prefix} Starting build for {num_elements} elements. Space='{space_val}', M={M_val}, efC={Efc_val}.\")\n",
    "    \n",
    "    idx = hnswlib.Index(space_val, dim)\n",
    "    idx.init_index(max_elements=num_elements, ef_construction=Efc_val, M=M_val, allow_replace_deleted=True)\n",
    "    \n",
    "    current_base_vectors = np.ascontiguousarray(base_vectors, dtype=np.float32)\n",
    "    \n",
    "    if hasattr(idx, 'set_num_threads'):\n",
    "        try:\n",
    "            idx.set_num_threads(N_THREADS)\n",
    "        except Exception as e:\n",
    "            print(f\"{log_prefix} Warning: Failed to set num_threads for HNSW build: {e}\")\n",
    "\n",
    "    idx.add_items(current_base_vectors, labels_arr)\n",
    "    print(f\"{log_prefix} HNSW index build fully complete.\")\n",
    "    return idx\n",
    "\n",
    "# (brute_force_gt_mt and eval_index functions are assumed to be the same as in the previous script)\n",
    "def brute_force_gt_mt(base_vectors: np.ndarray, query_vectors: np.ndarray, k_val: int, space_type: str, dname_for_log: str):\n",
    "    print(f\"    Calculating brute-force GT for {dname_for_log} (k={k_val}, space='{space_type}')...\")\n",
    "    num_queries, num_base = query_vectors.shape[0], base_vectors.shape[0]\n",
    "    k_val = min(k_val, num_base)\n",
    "    if k_val == 0: return np.array([], dtype=np.int32).reshape(num_queries, 0)\n",
    "    gt_indices = np.full((num_queries, k_val), -1, dtype=np.int32)\n",
    "    with ThreadPoolExecutor(max_workers=N_THREADS) as executor:\n",
    "        def process_query_chunk(start_idx, end_idx):\n",
    "            scores = _calculate_scores_for_chunk(query_vectors[start_idx:end_idx], base_vectors, space_type)\n",
    "            partition_indices = np.argpartition(scores, k_val, axis=1)[:, :k_val]\n",
    "            for i in range(partition_indices.shape[0]):\n",
    "                query_idx = start_idx + i\n",
    "                top_k_indices_for_query = partition_indices[i]\n",
    "                top_k_scores = scores[i, top_k_indices_for_query]\n",
    "                sorted_within_top_k = np.argsort(top_k_scores)\n",
    "                gt_indices[query_idx, :] = top_k_indices_for_query[sorted_within_top_k]\n",
    "        query_chunks = [(i, min(i + BF_CHUNK_SIZE, num_queries)) for i in range(0, num_queries, BF_CHUNK_SIZE)]\n",
    "        executor.map(lambda p: process_query_chunk(*p), query_chunks)\n",
    "    print(f\"    Brute-force GT calculation for {dname_for_log} complete.\")\n",
    "    return gt_indices\n",
    "def _calculate_scores_for_chunk(query_vectors_slice: np.ndarray, base_vectors_chunk: np.ndarray, space_type: str) -> np.ndarray:\n",
    "    if space_type == 'l2':\n",
    "        diff = query_vectors_slice[:, np.newaxis, :] - base_vectors_chunk[np.newaxis, :, :]\n",
    "        return np.linalg.norm(diff, axis=2)\n",
    "    elif space_type == 'cosine':\n",
    "        q_norm = normalize_vectors(query_vectors_slice)\n",
    "        b_norm = normalize_vectors(base_vectors_chunk)\n",
    "        if q_norm.size == 0 or b_norm.size == 0: return np.full((q_norm.shape[0], b_norm.shape[0]), np.inf, dtype=np.float32)\n",
    "        similarities = np.dot(q_norm, b_norm.T)\n",
    "        return 1.0 - np.clip(similarities, -1.0, 1.0)\n",
    "    raise ValueError(f\"Unsupported space_type for GT calculation: '{space_type}'\")\n",
    "def eval_index(idx: hnswlib.Index, query_vectors: np.ndarray, gt_indices: np.ndarray, ef_search: int, num_trials: int, recall_at_k: int):\n",
    "    if query_vectors.size == 0: return 0.0, 0.0\n",
    "    num_queries = len(query_vectors)\n",
    "    idx.set_ef(ef_search)\n",
    "    if gt_indices is None or gt_indices.shape[0] != num_queries:\n",
    "        total_time_no_gt = 0.0\n",
    "        for _ in range(num_trials):\n",
    "            start_time = time.time(); idx.knn_query(query_vectors, k=max(1, recall_at_k)); total_time_no_gt += time.time() - start_time\n",
    "        return 0.0, (num_trials * num_queries / total_time_no_gt) if total_time_no_gt > 0 else 0.0\n",
    "    k_to_evaluate = min(recall_at_k, gt_indices.shape[1])\n",
    "    if k_to_evaluate == 0: return 0.0, 0.0\n",
    "    gt_sets_for_eval = [set(row[:k_to_evaluate]) for row in gt_indices]\n",
    "    total_time_eval, total_correct_hits = 0.0, 0\n",
    "    for _ in range(num_trials):\n",
    "        start_time = time.time()\n",
    "        found_indices_batch, _ = idx.knn_query(query_vectors, k=k_to_evaluate)\n",
    "        total_time_eval += time.time() - start_time\n",
    "        for i in range(num_queries): total_correct_hits += len(set(found_indices_batch[i]) & gt_sets_for_eval[i])\n",
    "    qps = (num_trials * num_queries / total_time_eval) if total_time_eval > 0 else 0.0\n",
    "    total_possible_hits = num_trials * sum(len(s) for s in gt_sets_for_eval)\n",
    "    recall = (total_correct_hits / total_possible_hits) * 100.0 if total_possible_hits > 0 else 0.0\n",
    "    return recall, qps\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# ðŸš€ 5. MAIN EXPERIMENT LOOP\n",
    "# ===================================================================\n",
    "\n",
    "# Prepare CSV file\n",
    "csv_path = f\"{RESULTS_DIR}/experiment_results_consecutive_{TIMESTAMP}.csv\"\n",
    "csv_header = [\n",
    "    \"timestamp\", \"dataset\", \"total_delta\", \"n_consecutive_updates\", \"variant\", \"ef_search\",\n",
    "    \"M\", \"Efc\", \"rein\", \"space\", \"recall_k\",\n",
    "    \"qps\", \"recall\",\n",
    "    \"time_roe_total\", \"metric_roe_total\",\n",
    "    \"time_rdn_final\", \"metric_rdn_final\",\n",
    "    \"time_rea_final\", \"metric_rea_final\",\n",
    "    \"time_reinsert_total\",\n",
    "    \"time_idx_build_total\",\n",
    "    \"time_idx_update_total\",\n",
    "    \"base_size_used\", \"query_size_used\"\n",
    "]\n",
    "with open(csv_path, \"w\", newline=\"\") as fp:\n",
    "    csv.writer(fp).writerow(csv_header)\n",
    "\n",
    "print(f\"\\nðŸš€ Starting consecutive update experiments. Results will be saved to: {csv_path}\\n\")\n",
    "\n",
    "for dname_key, current_cfg in CFG.items():\n",
    "    print(f\"\\n{'='*20} Processing Dataset: {dname_key.upper()} ({current_cfg['space']}) {'='*20}\")\n",
    "    \n",
    "    # --- Data and GT Preparation ---\n",
    "    try:\n",
    "        base_data_full, Q_data_full, gt_provided_hdf5 = load_dataset(dname_key, DATA_ROOT_HDF5)\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR: Could not load dataset '{dname_key}': {e}. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    base_data_sliced = base_data_full[:current_cfg[\"base_size_limit\"]] if current_cfg[\"base_size_limit\"] is not None else base_data_full\n",
    "    actual_base_size = base_data_sliced.shape[0]\n",
    "    if actual_base_size == 0: continue\n",
    "    labels_for_index = np.arange(actual_base_size, dtype=np.int32)\n",
    "    \n",
    "    Q_data_sliced = Q_data_full[:current_cfg[\"query_size_limit\"]] if current_cfg[\"query_size_limit\"] is not None and Q_data_full is not None else Q_data_full\n",
    "    actual_query_size = Q_data_sliced.shape[0] if Q_data_sliced is not None else 0\n",
    "\n",
    "    if actual_query_size == 0:\n",
    "        print(f\"  WARNING: No query data for '{dname_key}'. Skipping evaluation part.\")\n",
    "        continue\n",
    "        \n",
    "    effective_recalculate_gt = RECALCULATE_GT or (current_cfg[\"base_size_limit\"] is not None)\n",
    "    gt_to_use = None\n",
    "    if effective_recalculate_gt:\n",
    "        gt_to_use = brute_force_gt_mt(base_data_sliced, Q_data_sliced, RECALL_K, current_cfg[\"space\"], dname_key)\n",
    "    elif gt_provided_hdf5 is not None:\n",
    "        gt_to_use = gt_provided_hdf5[:actual_query_size, :RECALL_K]\n",
    "    else:\n",
    "        gt_to_use = brute_force_gt_mt(base_data_sliced, Q_data_sliced, RECALL_K, current_cfg[\"space\"], dname_key)\n",
    "\n",
    "    if gt_to_use is None or gt_to_use.shape[0] != actual_query_size:\n",
    "        print(f\"  FATAL ERROR: Could not prepare Ground Truth for '{dname_key}'. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"  Data Ready: Base={actual_base_size}, Query={actual_query_size}, GT_K={gt_to_use.shape[1]}\")\n",
    "\n",
    "    for total_delta_val in DELETION_RATIOS:\n",
    "        print(f\"\\n-- Running for Dataset: {dname_key.upper()}, Total Deletion Ratio (Î´) = {total_delta_val} --\")\n",
    "        \n",
    "        # --- Prepare deletion chunks ---\n",
    "        num_to_delete_total = int(actual_base_size * total_delta_val)\n",
    "        if num_to_delete_total <= 0:\n",
    "            print(\"  Total items to delete is 0. Will evaluate initial index state only.\")\n",
    "            del_ids_chunks = [np.array([], dtype=np.int32)]\n",
    "        else:\n",
    "            del_ids_overall = np.random.choice(labels_for_index, num_to_delete_total, replace=False)\n",
    "            effective_n_updates = min(N_CONSECUTIVE_UPDATES, num_to_delete_total)\n",
    "            del_ids_chunks = np.array_split(del_ids_overall, effective_n_updates)\n",
    "        \n",
    "        print(f\"  Simulating {len(del_ids_chunks)} consecutive updates to reach Î´={total_delta_val}.\")\n",
    "\n",
    "        # --- Run for both variants (Original and PRO-HNSW) ---\n",
    "        for variant in [\"original\", \"pro-hnsw\"]:\n",
    "            print(f\"\\n  Processing Variant: {variant.upper()}...\")\n",
    "            \n",
    "            # 1. Build Initial Index\n",
    "            t_build_start = time.time()\n",
    "            idx = build_index(base_data_sliced, labels_for_index, current_cfg['M'], current_cfg['Efc'], current_cfg['space'], f\"{dname_key}-{variant}\")\n",
    "            time_idx_build = time.time() - t_build_start\n",
    "            print(f\"    {variant.upper()} index built in {time_idx_build:.4f}s\")\n",
    "            \n",
    "            # 2. Run Consecutive Updates\n",
    "            t_update_overall_start = time.time()\n",
    "            cumulative_metrics = {\"time_roe\": 0.0, \"metric_roe\": 0, \"time_reinsert\": 0.0}\n",
    "\n",
    "            for i, chunk_del_ids in enumerate(del_ids_chunks):\n",
    "                if len(chunk_del_ids) == 0: continue\n",
    "                \n",
    "                # Mark for deletion\n",
    "                for d_id in chunk_del_ids: idx.mark_deleted(int(d_id))\n",
    "                \n",
    "                # If optimized, run ROE for the current chunk\n",
    "                if variant == \"pro-hnsw\":\n",
    "                    t_roe_iter_start = time.time()\n",
    "                    removed_count_iter = idx.remove_obsolete_edges(chunk_del_ids.tolist())\n",
    "                    cumulative_metrics[\"time_roe\"] += (time.time() - t_roe_iter_start)\n",
    "                    cumulative_metrics[\"metric_roe\"] += int(removed_count_iter)\n",
    "                \n",
    "                # Re-insert the items\n",
    "                idx.set_ef(current_cfg['rein'])\n",
    "                reinsert_data = np.ascontiguousarray(base_data_sliced[chunk_del_ids], dtype=np.float32)\n",
    "                t_reinsert_iter_start = time.time()\n",
    "                idx.add_items(reinsert_data, chunk_del_ids.astype(np.int32), replace_deleted=True)\n",
    "                cumulative_metrics[\"time_reinsert\"] += (time.time() - t_reinsert_iter_start)\n",
    "\n",
    "            # 3. Run Final Repairs for PRO-HNSW\n",
    "            final_repair_metrics = {\"time_rdn\": 0.0, \"metric_rdn\": 0, \"time_rea\": 0.0, \"metric_rea\": 0}\n",
    "            if variant == \"pro-hnsw\" and num_to_delete_total > 0:\n",
    "                print(\"    Running final repair functions for PRO-HNSW...\")\n",
    "                t_rdn_start = time.time()\n",
    "                final_repair_metrics[\"metric_rdn\"] = int(idx.repair_disconnected_nodes())\n",
    "                final_repair_metrics[\"time_rdn\"] = time.time() - t_rdn_start\n",
    "\n",
    "                t_rea_start = time.time()\n",
    "                final_repair_metrics[\"metric_rea\"] = int(idx.resolve_edge_asymmetry())\n",
    "                final_repair_metrics[\"time_rea\"] = time.time() - t_rea_start\n",
    "                print(\"    Final repairs complete.\")\n",
    "\n",
    "            time_idx_update_total = time.time() - t_update_overall_start\n",
    "            print(f\"    {variant.upper()} all updates finished in {time_idx_update_total:.4f}s\")\n",
    "            \n",
    "            # 4. Evaluate and Save Results\n",
    "            print(f\"  Evaluating {variant.upper()} (total Î´={total_delta_val})...\")\n",
    "            for ef_search_val in current_cfg[\"efs_list\"]:\n",
    "                rec, qps = eval_index(idx, Q_data_sliced, gt_to_use, ef_search_val, TRIALS_LATENCY, RECALL_K)\n",
    "                print(f\"    {variant} ef={ef_search_val:3d}  R={rec:5.2f}%  QPS={qps:9.1f}\")\n",
    "                row_data = [TIMESTAMP, dname_key, total_delta_val, len(del_ids_chunks), variant, ef_search_val,\n",
    "                            current_cfg[\"M\"], current_cfg[\"Efc\"], current_cfg[\"rein\"], current_cfg[\"space\"], RECALL_K,\n",
    "                            f\"{qps:.3f}\", f\"{rec:.2f}\",\n",
    "                            f\"{cumulative_metrics['time_roe']:.4f}\", cumulative_metrics['metric_roe'],\n",
    "                            f\"{final_repair_metrics['time_rdn']:.4f}\", final_repair_metrics['metric_rdn'],\n",
    "                            f\"{final_repair_metrics['time_rea']:.4f}\", final_repair_metrics['metric_rea'],\n",
    "                            f\"{cumulative_metrics['time_reinsert']:.4f}\",\n",
    "                            f\"{time_idx_build:.4f}\", f\"{time_idx_update_total:.4f}\",\n",
    "                            actual_base_size, actual_query_size]\n",
    "                with open(csv_path, \"a\", newline=\"\") as fp:\n",
    "                    csv.writer(fp).writerow(row_data)\n",
    "\n",
    "            del idx\n",
    "            gc.collect()\n",
    "            time.sleep(1)\n",
    "\n",
    "print(f\"\\nâœ… All experiments complete. Final results have been saved to: {csv_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
