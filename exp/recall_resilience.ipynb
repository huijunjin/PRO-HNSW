{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383ff95e-f4a7-4f6e-addb-02f85e6d3469",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# HNSW Performance Degradation Experiment under Consecutive Updates\n",
    "\n",
    "This notebook simulates and evaluates the performance degradation of HNSW over a series of consecutive updates. It compares two variants:\n",
    "1.  **Original HNSW**: Standard deletion and re-insertion.\n",
    "2.  **PRO-HNSW (Iterative Repair)**: Applies the ROE, RDN, and REA repair modules within each update iteration.\n",
    "\n",
    "The primary metric for this experiment is Recall@K over the number of update iterations.\n",
    "\"\"\"\n",
    "\n",
    "# ===================================================================\n",
    "# ðŸ“ 1. CONFIGURATION SECTION\n",
    "# All user-configurable parameters are defined here.\n",
    "# ===================================================================\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Target Dataset and Paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "TARGET_DATASET_NAME = \"fashion-mnist-784-euclidean\"\n",
    "# NOTE: Assumes datasets are in a 'data/' subdirectory and results will be saved to a 'results/degradation' subdirectory.\n",
    "DATA_ROOT_HDF5 = \"../data\"\n",
    "OUTPUT_DIR = \"../results/resilience\"\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ HNSW and Experiment Parameters â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "M_PARAM = 8\n",
    "EF_CONSTRUCTION = 50\n",
    "REINSERT_EF = 25\n",
    "RECALL_K = 10\n",
    "EF_SEARCH_FOR_EVAL = 30  # A fixed efSearch value used for recall evaluation at each interval.\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Consecutive Update Simulation Parameters â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "PER_ITERATION_DELETE_RATIO = 0.001  # Ratio of the total dataset size to delete and re-insert in each iteration.\n",
    "TOTAL_UPDATE_ITERATIONS = 1000      # Total number of small-batch update iterations to perform.\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Evaluation Schedule â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Defines at which iteration points to stop and evaluate recall.\n",
    "EVALUATION_INTERVAL = 50  # Evaluate every 50 iterations.\n",
    "INITIAL_EVAL_POINTS = [0, 1, 5, 10, 20] # Also evaluate at these specific early iterations.\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ System Parameters â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "NUM_TRIALS_FOR_EVAL = 3\n",
    "NUM_THREADS_HNSW = 18\n",
    "\n",
    "# ===================================================================\n",
    "# ðŸ› ï¸ 2. SETUP & LIBRARY IMPORTS\n",
    "# ===================================================================\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "import numpy as np\n",
    "import hnswlib\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import gc\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Automatically generate evaluation points ---\n",
    "EVALUATION_ITERATIONS = sorted(list(set(INITIAL_EVAL_POINTS + list(range(0, TOTAL_UPDATE_ITERATIONS + 1, EVALUATION_INTERVAL)))))\n",
    "if TOTAL_UPDATE_ITERATIONS not in EVALUATION_ITERATIONS:\n",
    "    EVALUATION_ITERATIONS.append(TOTAL_UPDATE_ITERATIONS)\n",
    "print(f\"Evaluation will be performed at iterations: {EVALUATION_ITERATIONS}\")\n",
    "\n",
    "\n",
    "# --- Prepare results directory ---\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# ===================================================================\n",
    "# ðŸ”¬ 3. CORE HELPER FUNCTIONS\n",
    "# ===================================================================\n",
    "\n",
    "def normalize_l2(vectors: np.ndarray) -> np.ndarray:\n",
    "    if vectors is None or vectors.size == 0: return vectors\n",
    "    norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    return vectors / np.where(norms == 0, 1e-9, norms)\n",
    "\n",
    "def load_hdf5_data(data_root, dataset_name, recall_k):\n",
    "    file_path = os.path.join(data_root, f\"{dataset_name}.hdf5\")\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"ERROR: Dataset file not found at {file_path}\")\n",
    "        return None, None, None, None\n",
    "    try:\n",
    "        with h5py.File(file_path, 'r') as f:\n",
    "            base_vectors = np.array(f['train'], dtype=np.float32)\n",
    "            query_vectors = np.array(f['test'], dtype=np.float32)\n",
    "            gt_indices = np.array(f['neighbors'])\n",
    "            space = 'l2'\n",
    "            if 'angular' in dataset_name.lower() or 'cosine' in dataset_name.lower():\n",
    "                space = 'cosine'\n",
    "                base_vectors = normalize_l2(base_vectors)\n",
    "                query_vectors = normalize_l2(query_vectors)\n",
    "            \n",
    "            # Slice GT to the required K\n",
    "            if gt_indices.ndim == 2 and gt_indices.shape[1] > recall_k:\n",
    "                gt_indices = gt_indices[:, :recall_k]\n",
    "            \n",
    "            print(f\"Dataset '{dataset_name}' loaded: Base {base_vectors.shape}, Query {query_vectors.shape}, GT {gt_indices.shape}, Space: {space}\")\n",
    "            return base_vectors, query_vectors, gt_indices, space\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading HDF5 file {file_path}: {e}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "def build_hnsw_index(data: np.ndarray, labels: np.ndarray, m, efc, space, threads):\n",
    "    dim = data.shape[1]\n",
    "    p = hnswlib.Index(space=space, dim=dim)\n",
    "    p.init_index(max_elements=data.shape[0], ef_construction=efc, M=m, allow_replace_deleted=True)\n",
    "    if hasattr(p, 'set_num_threads'):\n",
    "        p.set_num_threads(threads)\n",
    "    p.add_items(data, labels)\n",
    "    return p\n",
    "\n",
    "def evaluate_recall(idx: hnswlib.Index, query_data: np.ndarray, gt_indices: np.ndarray, ef_search, recall_k, num_trials):\n",
    "    if query_data is None or gt_indices is None or query_data.shape[0] != gt_indices.shape[0]:\n",
    "        return 0.0\n",
    "    \n",
    "    num_queries = query_data.shape[0]\n",
    "    idx.set_ef(ef_search)\n",
    "    k_eval = min(recall_k, gt_indices.shape[1])\n",
    "    gt_sets = [set(row[:k_eval]) for row in gt_indices]\n",
    "\n",
    "    total_correct_hits = 0\n",
    "    for _ in range(num_trials):\n",
    "        labels_batch, _ = idx.knn_query(query_data, k=k_eval)\n",
    "        for i in range(num_queries):\n",
    "            found_valid = {label for label in labels_batch[i] if label != -1}\n",
    "            total_correct_hits += len(found_valid & gt_sets[i])\n",
    "            \n",
    "    total_gt_count = sum(len(s) for s in gt_sets) * num_trials\n",
    "    return (total_correct_hits / total_gt_count) * 100.0 if total_gt_count > 0 else 0.0\n",
    "\n",
    "# ===================================================================\n",
    "# ðŸš€ 4. MAIN EXPERIMENT LOGIC\n",
    "# ===================================================================\n",
    "\n",
    "print(f\"Starting HNSW degradation experiment for: {TARGET_DATASET_NAME}\")\n",
    "\n",
    "# --- Load Data ---\n",
    "base_vectors, query_vectors, ground_truth_indices, space_type = load_hdf5_data(DATA_ROOT_HDF5, TARGET_DATASET_NAME, RECALL_K)\n",
    "if base_vectors is None:\n",
    "    exit(\"Exiting due to data loading failure.\")\n",
    "\n",
    "# --- Prepare Update Chunks ---\n",
    "initial_base_size = base_vectors.shape[0]\n",
    "all_possible_labels = np.arange(initial_base_size, dtype=np.int32)\n",
    "items_per_iter = max(1, int(initial_base_size * PER_ITERATION_DELETE_RATIO))\n",
    "total_distinct_items_to_update = min(initial_base_size, TOTAL_UPDATE_ITERATIONS * items_per_iter)\n",
    "ids_for_entire_process = np.random.choice(all_possible_labels, size=total_distinct_items_to_update, replace=False)\n",
    "\n",
    "ids_chunks_for_iterations = []\n",
    "if total_distinct_items_to_update > 0:\n",
    "    effective_iterations = (total_distinct_items_to_update + items_per_iter - 1) // items_per_iter\n",
    "    ids_chunks_for_iterations = np.array_split(ids_for_entire_process, effective_iterations)\n",
    "\n",
    "print(f\"Simulation plan: {len(ids_chunks_for_iterations)} iterations with ~{items_per_iter} updates each.\")\n",
    "\n",
    "# --- Prepare CSV for Results ---\n",
    "csv_path = f\"{OUTPUT_DIR}/degradation_results_{TARGET_DATASET_NAME}_{STAMP}.csv\"\n",
    "csv_header = [\n",
    "    \"timestamp\", \"dataset\", \"variant\", \"iteration\", \"recall\", \"ef_search\",\n",
    "    \"M\", \"Efc\", \"rein\", \"recall_k\",\n",
    "    \"total_update_time\", \"total_roe_time\", \"total_rdn_time\", \"total_rea_time\",\n",
    "    \"total_removed_edges\", \"total_repaired_nodes\", \"total_resolved_edges\"\n",
    "]\n",
    "with open(csv_path, \"w\", newline=\"\") as fp:\n",
    "    csv.writer(fp).writerow(csv_header)\n",
    "\n",
    "# --- Run Experiment for each Variant ---\n",
    "variants_to_run = [\"Original HNSW\", \"PRO-HNSW (Iterative Repair)\"]\n",
    "final_plot_data = {}\n",
    "\n",
    "for variant_name in variants_to_run:\n",
    "    print(f\"\\n{'='*20} Processing Variant: {variant_name} {'='*20}\")\n",
    "\n",
    "    # 1. Build a fresh index for each variant\n",
    "    t_build_start = time.time()\n",
    "    hnsw_idx = build_hnsw_index(base_vectors, all_possible_labels, M_PARAM, EF_CONSTRUCTION, space_type, NUM_THREADS_HNSW)\n",
    "    build_time = time.time() - t_build_start\n",
    "    print(f\"  Initial index built in {build_time:.4f}s.\")\n",
    "\n",
    "    # 2. Initialize metrics and result storage\n",
    "    results_for_this_variant = []\n",
    "    cumulative_metrics = {\n",
    "        \"update_time\": 0.0, \"roe_time\": 0.0, \"rdn_time\": 0.0, \"rea_time\": 0.0,\n",
    "        \"removed_edges\": 0, \"repaired_nodes\": 0, \"resolved_edges\": 0\n",
    "    }\n",
    "    \n",
    "    # 3. Main update and evaluation loop\n",
    "    for iteration_idx in range(TOTAL_UPDATE_ITERATIONS + 1):\n",
    "        # --- Evaluate at scheduled intervals ---\n",
    "        if iteration_idx in EVALUATION_ITERATIONS:\n",
    "            recall_val = evaluate_recall(hnsw_idx, query_vectors, ground_truth_indices, EF_SEARCH_FOR_EVAL, RECALL_K, NUM_TRIALS_FOR_EVAL)\n",
    "            print(f\"  > Eval at Iteration {iteration_idx}: Recall@{RECALL_K} (efS={EF_SEARCH_FOR_EVAL}) = {recall_val:.2f}%\")\n",
    "            results_for_this_variant.append({'iterations': iteration_idx, 'recall': recall_val})\n",
    "            \n",
    "            # Write current state to CSV\n",
    "            row_data = [\n",
    "                datetime.now().strftime(\"%Y%m%d_%H%M%S\"), TARGET_DATASET_NAME, variant_name, iteration_idx, f\"{recall_val:.4f}\", EF_SEARCH_FOR_EVAL,\n",
    "                M_PARAM, EF_CONSTRUCTION, REINSERT_EF, space_type, RECALL_K,\n",
    "                f\"{cumulative_metrics['update_time']:.4f}\", f\"{cumulative_metrics['roe_time']:.4f}\", f\"{cumulative_metrics['rdn_time']:.4f}\", f\"{cumulative_metrics['rea_time']:.4f}\",\n",
    "                cumulative_metrics['removed_edges'], cumulative_metrics['repaired_nodes'], cumulative_metrics['resolved_edges']\n",
    "            ]\n",
    "            with open(csv_path, \"a\", newline=\"\") as fp:\n",
    "                csv.writer(fp).writerow(row_data)\n",
    "\n",
    "        # --- Perform update step (if not the last evaluation point) ---\n",
    "        if iteration_idx < len(ids_chunks_for_iterations):\n",
    "            ids_this_iteration = ids_chunks_for_iterations[iteration_idx]\n",
    "            if ids_this_iteration.size == 0:\n",
    "                continue\n",
    "\n",
    "            update_step_start_time = time.time()\n",
    "            \n",
    "            # a. Mark elements for deletion\n",
    "            for d_id in ids_this_iteration:\n",
    "                hnsw_idx.mark_deleted(int(d_id))\n",
    "            \n",
    "            # b. PRO-HNSW: Run ROE before re-insertion\n",
    "            if variant_name == \"PRO-HNSW (Iterative Repair)\":\n",
    "                t_roe_start = time.time()\n",
    "                removed_count = hnsw_idx.remove_obsolete_edges(ids_this_iteration.tolist())\n",
    "                cumulative_metrics[\"roe_time\"] += (time.time() - t_roe_start)\n",
    "                cumulative_metrics[\"removed_edges\"] += int(removed_count)\n",
    "            \n",
    "            # c. Re-insert elements\n",
    "            hnsw_idx.set_ef(REINSERT_EF)\n",
    "            reinsert_data = np.ascontiguousarray(base_vectors[ids_this_iteration], dtype=np.float32)\n",
    "            hnsw_idx.add_items(reinsert_data, ids_this_iteration.astype(np.int32), replace_deleted=True)\n",
    "            \n",
    "            # d. PRO-HNSW: Run RDN and REA after re-insertion\n",
    "            if variant_name == \"PRO-HNSW (Iterative Repair)\":\n",
    "                t_rdn_start = time.time()\n",
    "                repaired_count = hnsw_idx.repair_disconnected_nodes()\n",
    "                cumulative_metrics[\"rdn_time\"] += (time.time() - t_rdn_start)\n",
    "                cumulative_metrics[\"repaired_nodes\"] += int(repaired_count)\n",
    "\n",
    "                t_rea_start = time.time()\n",
    "                resolved_count = hnsw_idx.resolve_edge_asymmetry()\n",
    "                cumulative_metrics[\"rea_time\"] += (time.time() - t_rea_start)\n",
    "                cumulative_metrics[\"resolved_edges\"] += int(resolved_count)\n",
    "\n",
    "            cumulative_metrics[\"update_time\"] += (time.time() - update_step_start_time)\n",
    "            \n",
    "    final_plot_data[variant_name] = pd.DataFrame(results_for_this_variant)\n",
    "    del hnsw_idx\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"\\nâœ… All experiments complete. Results have been saved to: {csv_path}\")\n",
    "print(\"This CSV file can now be used to plot the performance degradation curves.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
